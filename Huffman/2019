Pioneers of Modern Computer Architecture Receive ACM A.M. Turing Award
Hennessy and Patterson's Foundational Contributions to Today's Microprocessors Helped Usher in Mobile and IoT Revolutions
NEW YORK, NY, March 21, 2018 - ACM, the Association for Computing Machinery, today named John L. Hennessy, former President of Stanford University, and David A. Patterson, retired Professor of the University of California, Berkeley, recipients of the 2017 ACM A.M. Turing Award for pioneering a systematic, quantitative approach to the design and evaluation of computer architectures with enduring impact on the microprocessor industry. Hennessy and Patterson created a systematic and quantitative approach to designing faster, lower power, and reduced instruction set computer (RISC) microprocessors. Their approach led to lasting and repeatable principles that generations of architects have used for many projects in academia and industry. Today, 99% of the more than 16 billion microprocessors produced annually are RISC processors, and are found in nearly all smartphones, tablets, and the billions of embedded devices that comprise the Internet of Things (IoT).

Hennessy and Patterson codified their insights in a very influential book, Computer Architecture: A Quantitative Approach, now in its sixth edition, reaching generations of engineers and scientists who have adopted and further developed their ideas. Their work underpins our ability to model and analyze the architectures of new processors, greatly accelerating advances in microprocessor design.

The ACM Turing Award, often referred to as the "Nobel Prize of Computing," carries a $1 million prize, with financial support provided by Google, Inc. It is named for Alan M. Turing, the British mathematician who articulated the mathematical foundation and limits of computing. Hennessy and Patterson will formally receive the 2017 ACM A.M. Turing Award at the ACM's annual awards banquet on Saturday, June 23, 2018 in San Francisco, California.

"ACM initiated the Turing Award in 1966 to recognize contributions of lasting and major technical importance to the computing field," said ACM President Vicki L. Hanson. "The work of Hennessy and Patterson certainly exemplifies this standard. Their contributions to energy-efficient RISC-based processors have helped make possible the mobile and IoT revolutions. At the same time, their seminal textbook has advanced the pace of innovation across the industry over the past 25 years by influencing generations of engineers and computer designers."

Attesting to the impact of Hennessy and Patterson's work is the assessment of Bill Gates, principal founder of Microsoft Corporation, that their contributions "have proven to be fundamental to the very foundation upon which an entire industry flourished."

Development of MIPS and SPARC
While the idea of reduced complexity architecture had been explored since the 1960s-most notably in the IBM 801 project-the work that Hennessy and Patterson led, at Stanford and Berkeley respectively, is credited with firmly establishing the feasibility of the RISC approach, popularizing its concepts, and introducing it to academia and industry. The RISC approach differed from the prevailing complex instruction set computer (CISC) computers of the time in that it required a small set of simple and general instructions (functions a computer must perform), requiring fewer transistors than complex instruction sets and reducing the amount of work a computer must perform.

Patterson's Berkeley team, which coined the term RISC, built and demonstrated their RISC-1 processor in 1982. With 44,000 transistors, the RISC-1 prototype outperformed a conventional CISC design that used 100,000 transistors. Hennessy co-founded MIPS Computer Systems Inc. in 1984 to commercialize the Stanford team's work. Later, the Berkeley team's work was commercialized by Sun Microsystems in its SPARC microarchitecture.

Despite initial skepticism of RISC by many computer architects, the success of the MIPS and SPARC entrepreneurial efforts, the lower production costs of RISC designs, as well as more research advances, led to wider acceptance of RISC. By the mid-1990s, RISC microprocessors were dominant throughout the field.

Groundbreaking Textbook
Hennessy and Patterson presented new scientifically-based methodologies in their 1990 textbook Computer Architecture: a Quantitative Approach. The book has influenced generations of engineers and, through its dissemination of key ideas to the computer architecture community, is credited with significantly increasing the pace of advances in microprocessor design. In Computer Architecture, Hennessy and Patterson encouraged architects to carefully optimize their systems to allow for the differing costs of memory and computation. Their work also enabled a shift from seeking raw performance to designing architectures that take into account issues such as energy usage, heat dissipation, and off-chip communication. The book was groundbreaking in that it was the first text of its kind to provide an analytical and scientific framework, as well as methodologies and evaluation tools for engineers and designers to evaluate the net value of microprocessor design.

Biographical Background

FATHERS OF THE DEEP LEARNING REVOLUTION RECEIVE ACM A.M. TURING AWARD
Bengio, Hinton, and LeCun Ushered in Major Breakthroughs in Artificial Intelligence
ACM named Yoshua Bengio, Geoffrey Hinton, and Yann LeCun recipients of the 2018 ACM A.M. Turing Award for conceptual and engineering breakthroughs that have made deep neural networks a critical component of computing. Bengio is Professor at the University of Montreal and Scientific Director at Mila, Quebec’s Artificial Intelligence Institute; Hinton is VP and Engineering Fellow of Google, Chief Scientific Adviser of The Vector Institute, and University Professor Emeritus at the University of Toronto; and LeCun is Professor at New York University and VP and Chief AI Scientist at Facebook.

Working independently and together, Hinton, LeCun and Bengio developed conceptual foundations for the field, identified surprising phenomena through experiments, and contributed engineering advances that demonstrated the practical advantages of deep neural networks. In recent years, deep learning methods have been responsible for astonishing breakthroughs in computer vision, speech recognition, natural language processing, and robotics—among other applications.

While the use of artificial neural networks as a tool to help computers recognize patterns and simulate human intelligence had been introduced in the 1980s, by the early 2000s, LeCun, Hinton and Bengio were among a small group who remained committed to this approach. Though their efforts to rekindle the AI community’s interest in neural networks were initially met with skepticism, their ideas recently resulted in major technological advances, and their methodology is now the dominant paradigm in the field.

The ACM A.M. Turing Award, often referred to as the “Nobel Prize of Computing,” carries a $1 million prize, with financial support provided by Google, Inc. It is named for Alan M. Turing, the British mathematician who articulated the mathematical foundation and limits of computing.

“Artificial intelligence is now one of the fastest-growing areas in all of science and one of the most talked-about topics in society,” said ACM President Cherri M. Pancake. “The growth of and interest in AI is due, in no small part, to the recent advances in deep learning for which Bengio, Hinton and LeCun laid the foundation. These technologies are used by billions of people. Anyone who has a smartphone in their pocket can tangibly experience advances in natural language processing and computer vision that were not possible just 10 years ago. In addition to the products we use every day, new advances in deep learning have given scientists powerful new tools—in areas ranging from medicine, to astronomy, to materials science.”

"Deep neural networks are responsible for some of the greatest advances in modern computer science, helping make substantial progress on long-standing problems in computer vision, speech recognition, and natural language understanding,” said Jeff Dean, Google Senior Fellow and SVP, Google AI. “At the heart of this progress are fundamental techniques developed starting more than 30 years ago by this year's Turing Award winners, Yoshua Bengio, Geoffrey Hinton, and Yann LeCun. By dramatically improving the ability of computers to make sense of the world, deep neural networks are changing not just the field of computing, but nearly every field of science and human endeavor."

Machine Learning, Neural Networks and Deep Learning

In traditional computing, a computer program directs the computer with explicit step-by-step instructions. In deep learning, a subfield of AI research, the computer is not explicitly told how to solve a particular task such as object classification. Instead, it uses a learning algorithm to extract patterns in the data that relate the input data, such as the pixels of an image, to the desired output such as the label “cat.” The challenge for researchers has been to develop effective learning algorithms that can modify the weights on the connections in an artificial neural network so that these weights capture the relevant patterns in the data.

Geoffrey Hinton, who has been advocating for a machine learning approach to artificial intelligence since the early 1980s, looked to how the human brain functions to suggest ways in which machine learning systems might be developed. Inspired by the brain, he and others proposed “artificial neural networks” as a cornerstone of their machine learning investigations.

In computer science, the term “neural networks” refers to systems composed of layers of relatively simple computing elements called “neurons” that are simulated in a computer. These “neurons,” which only loosely resemble the neurons in the human brain, influence one another via weighted connections. By changing the weights on the connections, it is possible to change the computation performed by the neural network. Hinton, LeCun and Bengio recognized the importance of building deep networks using many layers—hence the term “deep learning.”

The conceptual foundations and engineering advances laid by LeCun, Bengio and Hinton over a 30-year period were significantly advanced by the prevalence of powerful graphics processing unit (GPU) computers, as well as access to massive datasets. In recent years, these and other factors led to leap-frog advances in technologies such as computer vision, speech recognition and machine translation.

Hinton, LeCun and Bengio have worked together and independently. For example, LeCun performed postdoctoral work under Hinton’s supervision, and LeCun and Bengio worked together at Bell Labs beginning in the early 1990s. Even while not working together, there is a synergy and interconnectedness in their work, and they have greatly influenced each other.

Bengio, Hinton and LeCun continue to explore the intersection of machine learning with neuroscience and cognitive science, most notably through their joint participation in the Learning in Machines and Brains program, an initiative of CIFAR, formerly known as the Canadian Institute for Advanced Research.

Select Technical Accomplishments

The technical achievements of this year’s Turing Laureates, which have led to significant breakthroughs in AI technologies include, but are not limited to, the following:

Geoffrey Hinton

Backpropagation: In a 1986 paper, “Learning Internal Representations by Error Propagation,” co-authored with David Rumelhart and Ronald Williams, Hinton demonstrated that the backpropagation algorithm allowed neural nets to discover their own internal representations of data, making it possible to use neural nets to solve problems that had previously been thought to be beyond their reach. The backpropagation algorithm is standard in most neural networks today.

Boltzmann Machines: In 1983, with Terrence Sejnowski, Hinton invented Boltzmann Machines, one of the first neural networks capable of learning internal representations in neurons that were not part of the input or output.

Improvements to convolutional neural networks: In 2012, with his students, Alex Krizhevsky and Ilya Sutskever, Hinton improved convolutional neural networks using rectified linear neurons and dropout regularization. In the prominent ImageNet competition, Hinton and his students almost halved the error rate for object recognition and reshaped the computer vision field.

Yoshua Bengio

Probabilistic models of sequences: In the 1990s, Bengio combined neural networks with probabilistic models of sequences, such as hidden Markov models. These ideas were incorporated into a system used by AT&T/NCR for reading handwritten checks, were considered a pinnacle of neural network research in the 1990s, and modern deep learning speech recognition systems are extending these concepts.

High-dimensional word embeddings and attention: In 2000, Bengio authored the landmark paper, “A Neural Probabilistic Language Model,” that introduced high-dimension word embeddings as a representation of word meaning. Bengio’s insights had a huge and lasting impact on natural language processing tasks including language translation, question answering, and visual question answering. His group also introduced a form of attention mechanism which led to breakthroughs in machine translation and form a key component of sequential processing with deep learning.

Generative adversarial networks: Since 2010, Bengio’s papers on generative deep learning, in particular the Generative Adversarial Networks (GANs) developed with Ian Goodfellow, have spawned a revolution in computer vision and computer graphics. In one fascinating application of this work, computers can actually create original images, reminiscent of the creativity that is considered a hallmark of human intelligence.

Yann LeCun

Convolutional neural networks: In the 1980s, LeCun developed convolutional neural networks, a foundational principle in the field, which, among other advantages, have been essential in making deep learning more efficient. In the late 1980s, while working at the University of Toronto and Bell Labs, LeCun was the first to train a convolutional neural network system on images of handwritten digits. Today, convolutional neural networks are an industry standard in computer vision, as well as in speech recognition, speech synthesis, image synthesis, and natural language processing. They are used in a wide variety of applications, including autonomous driving, medical image analysis, voice-activated assistants, and information filtering.

Improving backpropagation algorithms: LeCun proposed an early version of the backpropagation algorithm (backprop), and gave a clean derivation of it based on variational principles. His work to speed up backpropagation algorithms included describing two simple methods to accelerate learning time.

Broadening the vision of neural networks: LeCun is also credited with developing a broader vision for neural networks as a computational model for a wide range of tasks, introducing in early work a number of concepts now fundamental in AI. For example, in the context of recognizing images, he studied how hierarchical feature representation can be learned in neural networks—a concept that is now routinely used in many recognition tasks. Together with Léon Bottou, he proposed the idea, used in every modern deep learning software, that learning systems can be built as complex networks of modules where backpropagation is performed through automatic differentiation. They also proposed deep learning architectures that can manipulate structured data, such as graphs.

ACM will present the 2018 A.M. Turing Award at its annual Awards Banquet on June 15 in San Francisco, California.

The A.M. Turing Award, the ACM's most prestigious technical award, is given for major contributions of lasting importance to computing.

This site celebrates all the winners since the award's creation in 1966. It contains biographical information, a description of their accomplishments, straightforward explanations of their fields of specialization, and text or video of their A. M. Turing Award Lecture.

John L. Hennessy
John L. Hennessy was President of Stanford University from 2000 to 2016. He is Director of the Knight-Hennessy Scholars Program at Stanford, a member of the Board of Cisco Systems and the Gordon and Betty Moore Foundation and Chairman of the Board of Alphabet Inc. Hennessy earned his Bachelor's degree in electrical engineering from Villanova University and his Master's and doctoral degrees in computer science from the State University of New York at Stony Brook.

Hennessy's numerous honors include the IEEE Medal of Honor, the ACM-IEEE CS Eckert-Mauchly Award (with Patterson), the IEEE John von Neumann Medal (with Patterson), the Seymour Cray Computer Engineering Award, and the Founders Award from the American Academy of Arts and Sciences. Hennessy is a Fellow of ACM and IEEE, and is a member of the National Academy of Engineering, the National Academy of Sciences and the American Philosophical Society.

David A. Patterson
David A. Patterson is a Distinguished Engineer at Google and serves as Vice Chair of the Board of the RISC-V Foundation, which offers an open free instruction set architecture with the aim to enable a new era of processor innovation through open standard collaboration. Patterson was Professor of Computer Science at UC, Berkeley from 1976 to 2016. He received his Bachelor's, Master's and doctoral degrees in computer science from the University of California, Los Angeles.

Patterson's numerous honors include the IEEE John von Neumann Medal (with Hennessy), the ACM-IEEE CS Eckert-Mauchly Award (with Hennessy), the Richard A. Tapia Award for Scientific Scholarship, Civic Science, and Diversifying Computing, and the ACM Karl V. Karlstrom Outstanding Educator Award. Patterson served as ACM President from 2004 to 2006. He is a Fellow of ACM, AAAS and IEEE, and was elected to the National Academy of Engineering and the National Academy of Sciences.


DARPA Announces 2019 Spectrum Collaboration Challenge Live Finale
DARPA’s three-year, $3.75 million challenge to solve spectrum scarcity will culminate in a live event alongside “Mobile World Congress Americas, in Partnership with CTIA”
OUTREACH@DARPA.MIL
1/30/2018
DARPA Announces 2019 Spectrum Collaboration Challenge Live Finale
DARPA today announced its plans for the 2019 finale of the Spectrum Collaboration Challenge (SC2)—the world’s first collaborative machine-intelligence competition to address spectrum scarcity. Through a new partnership with the GSMA, the organization that represents mobile operators worldwide, DARPA will host a live SC2 Championship event alongside the 2019 Mobile World Congress Americas (MWCA) annual conference and exhibition. The DARPA and GSMA partnership creates an opportunity to display the groundbreaking SC2 technologies for the wireless telecommunications community at an accessible, public event.

The three-year SC2 competition, already in progress, aims to autonomously manage the increasingly congested electromagnetic (EM) spectrum and unlock its full potential. Through SC2, teams from across the globe are competing to develop new access strategies and wireless paradigms in which radio networks autonomously collaborate and reason about how to make the most efficient use of the spectrum. MWCA attendees will be able to witness this first-of-its-kind competition that merges artificial intelligence, machine learning, and software-defined radio technology to help fundamentally rethink spectrum operations.

“The cause of SC2 and the potential worldwide implications of its resulting solutions could not be better served than by holding our finale on a global stage. Our partnership with the GSMA provides us with an opportunity to reach the community most impacted and inspired by the increasing challenges of spectrum scarcity,” said Paul Tilghman, the DARPA program manager leading the Challenge. “The event will create a platform for the SC2 teams to present three years’ worth of innovation, addressing spectrum challenges that have plagued commercial and military communications over the past two decades.”

“We’re excited to partner with DARPA on its Spectrum Collaboration Challenge and are thrilled they chose to hold the finals alongside Mobile World Congress Americas—the competition offers attendees additional opportunities to see innovation in action,” said Reed Peterson, Head of Mobile World Congress Americas, GSMA. “Particularly, as we enter the era of 5G and IoT, spectrum becomes even more essential in connecting billions of people and ‘things’, and we applaud DARPA’s efforts in discovering new ways to optimize spectrum usage.”

All facets of the mobile and wireless community—from large mobile operators and device manufacturers to public institutes and the military—are affected by the strain being placed on the EM spectrum as a result of the growing number of wireless-enabled devices, IoT, and beyond. “Our SC2 championship event seeks to bring together these diverse groups and excite them about a new possible future that helps autonomously expand spectrum access,” said Tilghman.

The first of SC2’s three phases concluded in an initial preliminary event on December 13, 2017, with the top-scoring teams each receiving $750,000 in prize money. The competition is now entering its second phase, where qualified teams from a new open track search will have a chance to compete alongside teams that moved on from the first phase during a second preliminary event, which will be held in December 2018. For the SC2 Championship event in 2019, the first, second, and third place finishers will have a chance to walk away with prizes of $2 million, $1 million, and $750,000, respectively. Judges will assess competitors based on their ability to produce a radio design that can most efficiently coexist within a congested wireless communications environment.

Teams interested in qualifying for a spot in the current phase of SC2 have until April 30, 2018 to successfully complete the qualification hurdles. Additional information about the phase two hurdles and SC2 can be found at: https://spectrumcollaborationchallenge.com/
